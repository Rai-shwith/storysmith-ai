"""
Test script for local model setup
Verifies that local models (Phi-3-mini and SDXL) work without API dependencies
"""

import os
import sys

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from dotenv import load_dotenv
load_dotenv()

def test_local_model_setup():
    """Test the local model setup for Phi-3-mini and SDXL"""
    
    print("üß™ Testing Local Model Setup")
    print("=" * 50)
    
    try:
        # Test 1: Import core components
        print("1Ô∏è‚É£  Testing core imports...")
        from langchain_core.runnables import Runnable
        from langchain_core.prompts import PromptTemplate
        print("   ‚úÖ LangChain core imports successful!")
        
        # Test 2: Test ML/AI library imports
        print("\n2Ô∏è‚É£  Testing ML library imports...")
        try:
            import torch
            print(f"   ‚úÖ PyTorch installed: {torch.__version__}")
            print(f"   üî• CUDA available: {torch.cuda.is_available()}")
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name()
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
                print(f"   üéÆ GPU device: {gpu_name}")
                print(f"   üíæ GPU memory: {gpu_memory_gb:.1f}GB")
                
                # GPU memory assessment
                if gpu_memory_gb < 4:
                    print("   ‚ö†Ô∏è  Limited GPU VRAM - may need to use CPU for image generation")
                elif gpu_memory_gb < 8:
                    print("   ‚úÖ Good GPU VRAM for most models")
                else:
                    print("   üöÄ Excellent GPU VRAM - perfect for SDXL and large models!")
        except ImportError as e:
            print(f"   ‚ùå PyTorch import failed: {e}")
            return False
            
        try:
            import transformers
            print(f"   ‚úÖ Transformers installed: {transformers.__version__}")
        except ImportError as e:
            print(f"   ‚ùå Transformers import failed: {e}")
            return False
            
        try:
            import diffusers
            print(f"   ‚úÖ Diffusers installed: {diffusers.__version__}")
        except ImportError as e:
            print(f"   ‚ùå Diffusers import failed: {e}")
            return False

        # Test 3: Test text generation model loading
        print("\n3Ô∏è‚É£  Testing text generation model...")
        try:
            from transformers import AutoTokenizer
            from config import TEXT_GENERATION_MODEL
            
            print(f"   üì• Loading tokenizer for: {TEXT_GENERATION_MODEL}")
            tokenizer = AutoTokenizer.from_pretrained(TEXT_GENERATION_MODEL)
            print(f"   ‚úÖ Tokenizer loaded successfully!")
            print(f"   üìù Vocab size: {len(tokenizer)}")
            
            # Test tokenization
            test_text = "Hello, how are you?"
            tokens = tokenizer.encode(test_text)
            print(f"   üß™ Test tokenization: '{test_text}' -> {len(tokens)} tokens")
            
        except Exception as e:
            print(f"   ‚ùå Text model test failed: {e}")
            print("   üí° Model will be downloaded on first use")
        
        # Test 4: Test image generation model loading (just check if it can be imported)
        print("\n4Ô∏è‚É£  Testing image generation setup...")
        try:
            from diffusers import StableDiffusionXLPipeline
            from config import IMAGE_GENERATION_MODEL
            
            print(f"   üì• Image model configured: {IMAGE_GENERATION_MODEL}")
            print(f"   ‚úÖ SDXL pipeline class imported successfully!")
            print("   üí° Image model will be downloaded on first use (~7GB)")
            
        except Exception as e:
            print(f"   ‚ùå Image model test failed: {e}")
            return False
        
        # Test 5: Test local configuration
        print("\n5Ô∏è‚É£  Testing local model configuration...")
        try:
            from config import USE_LOCAL_MODELS, LOCAL_MODEL_PATH, OUTPUT_DIR, TEMP_DIR
            
            print(f"   üè† USE_LOCAL_MODELS: {USE_LOCAL_MODELS}")
            print(f"   üìÅ LOCAL_MODEL_PATH: {LOCAL_MODEL_PATH}")
            print(f"   üìÅ OUTPUT_DIR: {OUTPUT_DIR}")
            print(f"   üìÅ TEMP_DIR: {TEMP_DIR}")
            
            # Check if directories exist
            dirs_to_check = [OUTPUT_DIR, TEMP_DIR]
            if USE_LOCAL_MODELS:
                dirs_to_check.append(LOCAL_MODEL_PATH)
                
            for dir_path in dirs_to_check:
                if os.path.exists(dir_path):
                    print(f"   ‚úÖ Directory exists: {dir_path}")
                else:
                    print(f"   üìÅ Directory will be created: {dir_path}")
                    
        except Exception as e:
            print(f"   ‚ùå Configuration test failed: {e}")
            return False
        
        # Test 6: Test story chain import
        print("\n6Ô∏è‚É£  Testing story chain setup...")
        try:
            from chains.story_chain import create_story_chain
            story_chain = create_story_chain()
            print("   ‚úÖ Story chain created successfully!")
            
            # Test prompt template
            prompt = story_chain.prompt_template.format(topic="a magical adventure")
            print(f"   üìù Prompt template working: {len(prompt)} characters")
            
        except Exception as e:
            print(f"   ‚ùå Story chain test failed: {e}")
            return False
        
        # Test 7: Test image utilities
        print("\n7Ô∏è‚É£  Testing image processing utilities...")
        try:
            from utils.image_merge import generate_image_from_prompt
            from PIL import Image
            print("   ‚úÖ Image processing utilities imported!")
            print("   üñºÔ∏è  Pillow (PIL) available for image processing")
            
        except Exception as e:
            print(f"   ‚ùå Image utilities test failed: {e}")
            return False
        
        # Test 8: Memory requirements check
        print("\n8Ô∏è‚É£  Checking system requirements...")
        try:
            import psutil
            
            # Get system memory
            memory = psutil.virtual_memory()
            memory_gb = memory.total / (1024**3)
            available_gb = memory.available / (1024**3)
            
            print(f"   üíæ Total RAM: {memory_gb:.1f}GB")
            print(f"   üíæ Available RAM: {available_gb:.1f}GB")
            
            if memory_gb < 6:
                print("   ‚ùå Warning: Less than 6GB RAM detected. Models may not run.")
            elif memory_gb < 8:
                print("   ‚ö†Ô∏è  Warning: Less than 8GB RAM detected. Models may run slowly.")
            elif memory_gb < 12:
                print("   ‚úÖ Good RAM for local model execution!")
                if torch.cuda.is_available():
                    print("   üéÆ GPU available - SDXL will use GPU VRAM instead of system RAM!")
            else:
                print("   ‚úÖ Excellent RAM for local model execution!")
                
        except ImportError:
            print("   ‚ÑπÔ∏è  psutil not available, skipping memory check")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Memory check failed: {e}")
        
        print("\nüéâ All tests passed! Local model setup is ready.")
        print("üí° You can now run: python main.py --test")
        print("üì• Models will be downloaded automatically on first use:")
        print("   ‚Ä¢ Phi-3-mini-4k-instruct (~2.4GB)")
        print("   ‚Ä¢ SDXL-base-1.0 (~7GB)")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Test failed with error: {e}")
        return False


def test_quick_generation():
    """Test a quick text generation to verify everything works"""
    print("\nüöÄ Quick Generation Test")
    print("=" * 30)
    
    try:
        from chains.story_chain import create_story_chain
        
        print("Creating story chain...")
        story_chain = create_story_chain()
        
        print("Testing with a simple topic...")
        result = story_chain.invoke({"topic": "a friendly robot"})
        
        if result and "result" in result:
            story_data = result["result"]
            print("‚úÖ Story generation successful!")
            print(f"üìñ Story preview: {story_data.get('story', '')[:100]}...")
        else:
            print("‚ùå Story generation failed - no result returned")
            return False
            
        return True
        
    except Exception as e:
        print(f"‚ùå Quick generation test failed: {e}")
        print("üí° This is normal if models haven't been downloaded yet")
        return False


if __name__ == "__main__":
    print("üîß StorySmith AI - Local Model Setup Test")
    print("=" * 60)
    
    # Run main setup test
    setup_success = test_local_model_setup()
    
    if setup_success:
        print("\n" + "=" * 60)
        print("‚ú® Setup test completed successfully!")
        
        # Ask if user wants to run quick generation test
        try:
            user_input = input("\nü§î Run quick generation test? (y/N): ").strip().lower()
            if user_input in ['y', 'yes']:
                generation_success = test_quick_generation()
                if generation_success:
                    print("\nüéâ All tests passed! Ready for local story generation!")
                else:
                    print("\n‚ö†Ô∏è  Generation test failed, but setup is ready.")
                    print("üí° Models will download on first actual use.")
            else:
                print("\n‚úÖ Setup verified! Run 'python main.py' to start generating stories.")
                
        except KeyboardInterrupt:
            print("\nüëã Test interrupted by user")
    else:
        print("\nüîß Please fix the issues above before proceeding.")
        print("\nüí° Common fixes:")
        print("   ‚Ä¢ Run: pip install -r requirements.txt")
        print("   ‚Ä¢ Ensure you have sufficient RAM (8GB+ recommended)")
        print("   ‚Ä¢ Check that Python packages are properly installed")
